{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As a note, many different imports are used for running the various results and models. ClarkeErrorGrid is a separate .py file that is imported in order to produce the clarke error grid graphs. ClarkeErrorGrid.py can be found in the same github repository as this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "import numpy.linalg as la\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import xml.etree.ElementTree as et \n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.stattools import arma_order_select_ic as order_selecte\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "\n",
    "from filterpy.kalman import KalmanFilter\n",
    "from filterpy.kalman import UnscentedKalmanFilter\n",
    "from filterpy.common import Q_discrete_white_noise\n",
    "from filterpy.kalman import MerweScaledSigmaPoints\n",
    "\n",
    "from scipy.stats.distributions import norm\n",
    "from scipy.optimize import fmin\n",
    "\n",
    "from pydataset import data as pydata\n",
    "from collections import defaultdict\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from ClarkeErrorGrid import clarke_error_grid\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import TimeDistributed    \n",
    "\n",
    "from fbprophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_labels = [\"RMSE\",\"MAE\",\"TP_hypo\",\"FP_hypo\",\"TN_hypo\",\"FN_hypo\",\"TP_hyper\",\"FP_hyper\",\"TN_hyper\",\"FN_hypter\",\"MC_hypo\",\"MC_hyper\",\"R2\",\"Zone A\", \"Zone B\", \"Zone C\", \"Zone D\", \"Zone E\", \"Perc. Safe\", \"Perc. Unnecessary\", \"Perc. Dangerous\"]\n",
    "def get_metrics(actual,predicted):\n",
    "    \"\"\"\n",
    "    Get metrics return as tuple regarding the various metrics we wish to use when analyzing how well an algorithm is predicting\n",
    "    \n",
    "    Parameters:\n",
    "        actual(ndarray, (n,)): The actual blood glucose levels for a given time series\n",
    "        predicted (ndarray, (n)): The predicted blood glucose levels for a given time series\n",
    "    \n",
    "    Returns:\n",
    "        RMSE: The Root Mean Square Error of the prediction\n",
    "        MAE: The Mean Absolute Error of the prediction\n",
    "        TP_hypo: The true positive rate for predicted Hypoglycaemic (<70) events\n",
    "        FP_hypo: The false positive rate for predicted Hypoglycaemic (<70) events\n",
    "        TN_hypo: The true negative rate for predicted hypoglycaemic (< 70) events\n",
    "        FN_hypo: The false negative rate for predicted hypoglycaemic (< 70) events\n",
    "        TP_hyper: The true positive rate for predicted Hyperglycaemic events (>180)\n",
    "        FP_hyper: The false positive rate for predicted hyperglycaemic events (>180)\n",
    "        TN_hyper: The true negative rate for predicted hyperglycaemic events(> 180)\n",
    "        FN_hypter: The false negative rate for predicted hyperglycaemic events (>180)\n",
    "        MC_hypo: The Matthew Correlation Coefficient predicted for hypoglycaemic (<70) events\n",
    "        MC_hyper: The Matthew Correlation Coefficient predicted for hyperglycaemic (> 180) events\n",
    "        r2: The R^2 score for the given predictions\n",
    "    \"\"\"\n",
    "    RMSE = mean_squared_error(actual,predicted,squared=False)\n",
    "    MAE = mean_absolute_error(actual,predicted)\n",
    "    actual_hypo = np.where(actual < 70,1,0)\n",
    "    actual_hyper = np.where(actual > 180,1,0)\n",
    "    predicted_hypo = np.where(predicted < 70,1,0)\n",
    "    predicted_hyper = np.where(predicted > 180,1,0)\n",
    "    tn_hypo,fp_hypo,fn_hypo,tp_hypo = confusion_matrix(actual_hypo,predicted_hypo).ravel()\n",
    "    mc_hypo = matthews_corrcoef(actual_hypo,predicted_hypo)\n",
    "    tn_hyper,fp_hyper,fn_hyper,tp_hyper = confusion_matrix(actual_hyper,predicted_hyper).ravel()\n",
    "    mc_hyper = matthews_corrcoef(actual_hyper,predicted_hyper)\n",
    "    r2 = r2_score(actual,predicted)\n",
    "    return RMSE, MAE, tp_hypo,fp_hypo,tn_hypo,fn_hypo,tp_hyper,fp_hyper,tn_hyper,fn_hyper,mc_hypo,mc_hyper,r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code makes the assumption that you already have access to the OhioT1DM dataset, and that the corresponding data (as of 2020) is found in the folder ./OHIODATA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_XML(xml_file): \n",
    "    \"\"\"Parse the input XML file and store the result in a pandas \n",
    "    DataFrame with the given columns. \n",
    "    \n",
    "    The first element of df_cols is supposed to be the identifier \n",
    "    variable, which is an attribute of each node element in the \n",
    "    XML data; other features will be parsed from the text content \n",
    "    of each sub-element. \n",
    "    \"\"\"\n",
    "    #use the columns as defined in the OhioT1DM Dataset 2020\n",
    "    df_cols = [\"date\",\"glucose_level\", \"finger_stick\", \"basal\", \"temp_basal\",\"bolus\",\"meal\",\"sleep\",\n",
    "           \"work\",\"stressors\",\"hypo_event\",\"illness\",\"exercise\",\"basis_heart_rate\",\"basis_gsr\",\n",
    "          \"basis_skin_temperature\",\"basis_air_temperature\",\"basis_steps\",\"basis_sleep\",\"acceleration\"]\n",
    "    \n",
    "    xtree = et.parse(xml_file)\n",
    "    xroot = xtree.getroot()\n",
    "    rows = []\n",
    "    \n",
    "    #Goes row by row from the XML file and appends to an array which will be converted to a PANDAS dataframe\n",
    "    for child in xroot:\n",
    "        if child.tag == df_cols[1]:\n",
    "            final = []\n",
    "            for val in child.getchildren():\n",
    "                res = [np.nan] * 20\n",
    "                res[0] = str(val.get(\"ts\"))\n",
    "                res[1] = float(val.get(\"value\"))\n",
    "                final.append(res)\n",
    "            \n",
    "        elif child.tag == df_cols[2]:\n",
    "            for val in child.getchildren():\n",
    "                res = [np.nan] * 20\n",
    "                res[0] = str(val.get(\"ts\"))\n",
    "                res[2] = float(val.get(\"value\"))\n",
    "                final.append(res)\n",
    "\n",
    "        elif child.tag == df_cols[3]:\n",
    "            for val in child.getchildren():\n",
    "                res = [np.nan] * 20\n",
    "                res[0] = str(val.get(\"ts\"))\n",
    "                res[3] = float(val.get(\"value\"))\n",
    "                final.append(res)\n",
    "                \n",
    "        elif child.tag == df_cols[4]:\n",
    "            for val in child.getchildren():\n",
    "                res = [np.nan] * 20\n",
    "                res[0] = str(val.get(\"ts_begin\"))\n",
    "                res[4] = float(val.get(\"value\"))\n",
    "                final.append(res)\n",
    "\n",
    "        elif child.tag == df_cols[5]:\n",
    "            for val in child.getchildren():\n",
    "                res = [np.nan] * 20\n",
    "                res[0] = str(val.get(\"ts_begin\"))\n",
    "                res[5] = float(val.get(\"dose\"))\n",
    "                final.append(res)\n",
    "                \n",
    "        elif child.tag == \"meal\":\n",
    "            for val in child.getchildren():\n",
    "                res = [np.nan] * 20\n",
    "                res[0] = str(val.get(\"ts\"))\n",
    "                res[6] = float(val.get(\"carbs\"))\n",
    "                final.append(res)\n",
    "                \n",
    "        elif child.tag == \"sleep\":\n",
    "            for val in child.getchildren():\n",
    "                res = [np.nan] * 20\n",
    "                res[0] = str(val.get(\"tbegin\"))\n",
    "                res[7] = np.nan  #FOR THIS FIND THE DIFFERENCE BETWEEN tbegin AND tend\n",
    "                final.append(res)        \n",
    "        elif child.tag == \"work\":\n",
    "            for val in child.getchildren():\n",
    "                res = [np.nan] * 20\n",
    "                res[0] = str(val.get(\"tbegin\"))\n",
    "                res[8] = np.nan  #FOR THIS FIND THE DIFFERENCE BETWEEN tbegin AND tend\n",
    "                final.append(res)        \n",
    "        elif child.tag == \"stressors\":\n",
    "            for val in child.getchildren():\n",
    "                res = [np.nan] * 20\n",
    "                res[0] = str(val.get(\"ts\"))\n",
    "                res[9] = 1\n",
    "                final.append(res)        \n",
    "        elif child.tag == \"hypo_event\":\n",
    "            for val in child.getchildren():\n",
    "                res = [np.nan] * 20\n",
    "                res[0] = str(val.get(\"ts\"))\n",
    "                res[10] = np.nan \n",
    "                final.append(res)  \n",
    "        elif child.tag == \"illness\":\n",
    "            for val in child.getchildren():\n",
    "                res = [np.nan] * 20\n",
    "                res[0] = str(val.get(\"ts_begin\"))\n",
    "                res[11] = np.nan\n",
    "                final.append(res)  \n",
    "        elif child.tag == \"exercise\":\n",
    "            for val in child.getchildren():\n",
    "                res = [np.nan] * 20\n",
    "                res[0] = str(val.get(\"ts\"))\n",
    "                res[12] = float(val.get(\"intensity\")) * float(val.get(\"duration\"))\n",
    "                final.append(res)  \n",
    "        elif child.tag == \"basis_heart_rate\":\n",
    "            print(\"basis_heart_rate not included\")\n",
    "        elif child.tag == \"basis_gsr\":\n",
    "            for val in child.getchildren():\n",
    "                res = [np.nan] * 20\n",
    "                res[0] = str(val.get(\"ts\"))\n",
    "                res[14] = float(val.get(\"value\"))\n",
    "                final.append(res)\n",
    "        elif child.tag == \"basis_skin_temperature\":\n",
    "            for val in child.getchildren():\n",
    "                res = [np.nan] * 20\n",
    "                res[0] = str(val.get(\"ts\"))\n",
    "                res[15] = float(val.get(\"value\"))\n",
    "                final.append(res)\n",
    "        elif child.tag == \"basis_air_temperature\":\n",
    "            print(\"basis_air_temperature not included\")\n",
    "        elif child.tag == \"basis_steps\":\n",
    "            print(\"basis_steps not included\")\n",
    "        elif child.tag == \"basis_sleep\":\n",
    "            for val in child.getchildren():\n",
    "                res = [np.nan] * 20\n",
    "                res[0] = str(val.get(\"tbegin\"))\n",
    "                res[18] = np.nan  #FOR THIS FIND THE DIFFERENCE BETWEEN tbegin AND tend\n",
    "                final.append(res)\n",
    "        elif child.tag == \"acceleration\":\n",
    "            for val in child.getchildren():\n",
    "                res = [np.nan] * 20\n",
    "                res[0] = str(val.get(\"ts\"))\n",
    "                res[19] = float(val.get(\"value\"))\n",
    "                final.append(res)\n",
    "      \n",
    "    #CONVERT ARRAY TO PANDAS DATAFRAME\n",
    "    df = pd.DataFrame(final, columns = df_cols)\n",
    "    df[df['date'] == \"None\"] = np.nan\n",
    "    df.dropna(subset=[\"date\"],inplace=True)\n",
    "    \n",
    "    #convert date column into a datetime column, then remove date \n",
    "    df['datetime'] = pd.to_datetime(df['date'],format='%d-%m-%Y %H:%M:%S')\n",
    "    df = df.set_index('datetime')\n",
    "    df.drop(['date'], axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    #Since the array is mostly sparse and not aligned by time, merging the dataframes to align multiple data values allows for better multivariate analysis using time series\n",
    "    #The first block initializes the new dataframe merging, the following loop goes through the rest of the columns\n",
    "    #Each merge is done backward since we can't use data from the future to predict the future.\n",
    "    #https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.merge_asof.html\n",
    "    glu = df[\"glucose_level\"].dropna()\n",
    "    finger = df[\"finger_stick\"].dropna()\n",
    "    new_df = pd.merge_asof(glu, finger,\n",
    "                          on='datetime',\n",
    "                          direction = \"backward\",\n",
    "                          tolerance=pd.Timedelta('4T'))\n",
    "    df_cols = [\"basal\", \"temp_basal\",\"bolus\",\"meal\",\"sleep\",\n",
    "               \"work\",\"stressors\",\"hypo_event\",\"illness\",\"exercise\",\"basis_heart_rate\",\"basis_gsr\",\n",
    "              \"basis_skin_temperature\",\"basis_air_temperature\",\"basis_steps\",\"basis_sleep\",\"acceleration\"]\n",
    "    \n",
    "    for col in df_cols:\n",
    "        temp = df[col].dropna()\n",
    "        new_df = pd.merge_asof(new_df, temp,\n",
    "                          on='datetime',\n",
    "                          direction = \"backward\",\n",
    "                          tolerance=pd.Timedelta('4T'))\n",
    "    new_df = new_df.set_index(['datetime'])\n",
    "    \n",
    "    practice = new_df    #need to check where datetime is missing values, then extrapolate\n",
    "    practice['deltaT'] = practice.index.to_series().diff().dt.total_seconds().div(60, fill_value=0)\n",
    "    i = 1\n",
    "    filled_dates = []\n",
    "    index = practice.index.values\n",
    "    index = pd.to_datetime(index)\n",
    "\n",
    "    #FILLS MISSING VALUES: uses a rolling average \n",
    "    while i < len(practice)-1:\n",
    "        if practice[\"deltaT\"].values[i] > 6:\n",
    "            start = index[i-1]      \n",
    "            end = index[i]          \n",
    "            last_date = start\n",
    "\n",
    "            j = 1\n",
    "            while last_date < end:\n",
    "                last_date = last_date + timedelta(minutes=5)\n",
    "\n",
    "                pred = np.mean(practice[\"glucose_level\"].values[i-2*j:i-1])   \n",
    "                j+=1\n",
    "                filled_dates.append([last_date,pred,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None])\n",
    "        i += 1\n",
    "\n",
    "    #REMAKES THE DATAFRAME WITH FILLED DATES\n",
    "    temp_df = pd.DataFrame(filled_dates, columns = [\"date\",\"glucose_level\", \"finger_stick\", \"basal\", \"temp_basal\",\"bolus\",\"meal\",\"sleep\",\n",
    "           \"work\",\"stressors\",\"hypo_event\",\"illness\",\"exercise\",\"basis_heart_rate\",\"basis_gsr\",\n",
    "          \"basis_skin_temperature\",\"basis_air_temperature\",\"basis_steps\",\"basis_sleep\",\"acceleration\"])\n",
    "    temp_df['datetime'] = pd.to_datetime(temp_df['date'],format='%d-%m-%Y %H:%M:%S')\n",
    "    temp_df = temp_df.set_index('datetime')\n",
    "    temp_df.drop(['date'], axis=1, inplace=True)\n",
    "\n",
    "    #ALIGNS THE INDEX BY TIME\n",
    "    final_df = pd.concat([practice,temp_df])\n",
    "    new_df = final_df.sort_index()\n",
    "    #remove nans, and fill remaining with 0\n",
    "    new_df.dropna(subset = [\"glucose_level\"], inplace = True)\n",
    "    new_df = new_df.fillna(0)\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_file(metric_path,metrics_dict):\n",
    "    \"\"\"\n",
    "    Creates a metric file containing the results of the get_metrics function for each patient for a given model\n",
    "    \n",
    "    Parameters:\n",
    "        metric_path(str):The file path for the metric file\n",
    "        metrics_dict(dictionary):Dictionary containing the results of get_metrics for each patient\n",
    "    \"\"\"\n",
    "    with open(metric_path,\"w+\") as outfile:\n",
    "        for key in metrics_dict.keys():\n",
    "            outfile.write(\"Patient \" + str(key) + \"\\n\")\n",
    "            for met,val in zip(metric_labels,metrics_dict[key]):\n",
    "                outfile.write(met + \": \" + str(val) + \"\\n\")\n",
    "            outfile.write(\"\\n\")\n",
    "        rmse = np.mean([val[0] for val in metrics_dict.values()])\n",
    "        mae = np.mean([val[1] for val in metrics_dict.values()])\n",
    "        mcc_l = np.mean([val[10] for val in metrics_dict.values()])\n",
    "        mcc_h  = np.mean([val[11] for val in metrics_dict.values()])\n",
    "        r2 = np.mean([val[12] for val in metrics_dict.values()])\n",
    "        outfile.write(\"Average RMSE: \" + str(rmse) + \"\\n\")\n",
    "        outfile.write(\"Average MAE: \" + str(mae) + \"\\n\")\n",
    "        outfile.write(\"Average MCC_hypo: \" + str(mcc_l) + \"\\n\")\n",
    "        outfile.write(\"Average MCC_hyper: \" + str(mcc_h) + \"\\n\")\n",
    "        outfile.write(\"Average R^2: \" + str(r2) + \"\\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directories(root,leafs):\n",
    "    \"\"\"\n",
    "    Creates all need directories for creating files.\n",
    "    Parameters:\n",
    "        root(str):the name of the root directory to create\n",
    "        leafs(list):list containing the leaf nodes to create\n",
    "    Returns:\n",
    "        None\n",
    "    Example:\n",
    "    root='pickle'\n",
    "    leafs=['linear','ridge','svm/rbf']\n",
    "    \n",
    "    This above example with root 'pickle' and the given leafs will create the following filepath and directories:\n",
    "    pickle/30min/linear, pickle/30min/ridge, pickle/30min/svm/rbf, pickle/60min/linear, pickle/60min/ridge, pickle/60min/svm/rbf\n",
    "    \n",
    "    \"\"\"\n",
    "    base30 = root + \"/30min/\"\n",
    "    base60 = root + \"/60min/\"\n",
    "    for leaf in leafs:\n",
    "        path30 = base30 + leaf\n",
    "        path60 = base60 + leaf\n",
    "        if not os.path.exists(path30):\n",
    "            os.makedirs(path30)\n",
    "        if not os.path.exists(path60):\n",
    "            os.makedirs(path60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pickle_dir():\n",
    "    \"\"\"\n",
    "    Creates the pickle directory, along with all needed subdirectories\n",
    "    \"\"\"\n",
    "    root = \"pickle\"\n",
    "    leafs = [\"linear\",\"ridge\",\"lasso\",\"elastic\",\"svm/poly\",\"svm/sigmoid/\",\"svm/rbf\",\"knn\",\"rf\",\"gradient\",\"xgb\",\"mlp\"]\n",
    "    create_directories(root,leafs)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plot_dir():\n",
    "    \"\"\"\n",
    "    Creates plots directory, along with all needed subdirectories\n",
    "\n",
    "    \"\"\"\n",
    "    root = \"plots\"\n",
    "    leafs = [\"linear\",\"ridge\",\"lasso\",\"elastic\",\"svm/poly\",\"svm/sigmoid/\",\"svm/rbf\",\"knn\",\"rf\",\"gradient\",\"xgb\",\"mlp\"]\n",
    "    create_directories(root,leafs)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clarke_dir():\n",
    "    \"\"\"\n",
    "    Creates the clarke directory, along with all needed subdirectories\n",
    "    \"\"\"\n",
    "    root = \"clarke\"\n",
    "    leafs = [\"linear\",\"ridge\",\"lasso\",\"elastic\",\"svm/poly\",\"svm/sigmoid/\",\"svm/rbf\",\"knn\",\"rf\",\"gradient\",\"xgb\",\"mlp\"]\n",
    "    create_directories(root,leafs)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_dir():\n",
    "    \"\"\"\n",
    "    Creates the metrics directory, along with all needed subdirectories\n",
    "    \"\"\"\n",
    "    root = \"metrics\"\n",
    "    leafs = [\"linear\",\"ridge\",\"lasso\",\"elastic\",\"svm/poly\",\"svm/sigmoid/\",\"svm/rbf\",\"knn\",\"rf\",\"gradient\",\"xgb\",\"mlp\"]\n",
    "    create_directories(root,leafs)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_directories():\n",
    "    \"\"\"\n",
    "    Creates the pickle,plots,clarke, and metrics directories, along with all needed subdirectories\n",
    "\n",
    "    \"\"\"\n",
    "    create_pickle_dir()\n",
    "    create_plot_dir()\n",
    "    create_clarke_dir()\n",
    "    create_metrics_dir()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_plot(xvals,actual,pred,plot_title,filepath):\n",
    "    \"\"\"\n",
    "    Creates a plot of the predicted blood glucose values against the actual blood glucose values. Saves the plot in a \n",
    "    figure\n",
    "    Parameters:\n",
    "        xvals(list): Range for plotting the glucose values. Since data is time indexed, this is just an list from 0 to the number of \n",
    "        blood glucose values\n",
    "        actual((n,) ndarray): Array containing the actual blood glucose values\n",
    "        pred((n,) ndarray): Array containing the predicted blood glucose values\n",
    "        plot_title(str): String for the name of the plot\n",
    "        filepath(str): File path for storing the plot\n",
    "    \"\"\"\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.plot(xvals,actual,'ob',label=\"actual\",ms=0.5)\n",
    "    ax.plot(xvals,pred,'or',label=\"predicted\",ms=0.5,linewidth=1)\n",
    "    ax.set_xlabel(\"time step\")\n",
    "    ax.set_ylabel(\"blood glucose (mg/dl)\")\n",
    "    ax.legend()\n",
    "    plt.savefig(filepath,bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_train_dictionary():\n",
    "    \"\"\"\n",
    "    Creates a map that maps the patient number to the test and train data for that patient\n",
    "    Returns:\n",
    "        test_train_dict(dictionary): dictionary mapping patient number to the test and train dataframes\n",
    "    \"\"\"\n",
    "    #as mentioned earlier, the code assumes that you already have the OhioT1DM dataset and that this data is in the folder OHIODATA\n",
    "    #as well, this code is specifically written for the 2020 OhioT1DM dataset\n",
    "    test_train_dict = {}\n",
    "    base = \"OHIODATA/\"\n",
    "    test=\"OhioT1DM-2-testing/\"\n",
    "    train = \"OhioT1DM-2-training/\"\n",
    "    end_test = \"-ws-testing.xml\"\n",
    "    end_train = \"-ws-training.xml\"\n",
    "    file_nums = [540,544,552,567,584,596]\n",
    "    for num in file_nums:\n",
    "        file = base + test+str(num)+end_test\n",
    "        test_data = parse_XML(file)\n",
    "        train_data = parse_XML(base+train+str(num)+end_train)\n",
    "        test_train_dict[num] = {\"test\":test_data,\"train\":train_data}\n",
    "    return test_train_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_regression(num_lags,pred_length,df,features=None):\n",
    "    \"\"\"\n",
    "    Takes the time series data and turns the data into a regression problem by converting the previous timesteps\n",
    "    into features and making label the next pred_length value of blood glucose values\n",
    "    Parameters:\n",
    "        num_lags(int): How many time steps (in 5 minutes) should be used to create the features\n",
    "        pred_length(int): How far into the future (in 5 minute steps) should the label be\n",
    "        df(pandas dataframe): dataframe containing the patient's information\n",
    "        features(list or None): Which columns from the dataframe should be included when creating the features, if\n",
    "            None, then use all columns\n",
    "    \"\"\"\n",
    "    #total number of samples is the lag length + prediction length +1 since 0 indexed\n",
    "    pred_tot = len(df)-(num_lags+pred_length-1)\n",
    "    #create the corresponding labels\n",
    "    gluc_values = df[\"glucose_level\"].values\n",
    "    labels=np.zeros((pred_tot,pred_length))\n",
    "    for i in range(pred_length):\n",
    "        labels[:,i] = gluc_values[num_lags+i:num_lags+pred_tot+i]\n",
    "    if features is None:\n",
    "        #transform for all columns\n",
    "        features=df.columns.values\n",
    "    first=True\n",
    "    #create the features\n",
    "    for feature in features:\n",
    "        if(feature == \"deltaT\"):\n",
    "            continue\n",
    "        feature_vals = df[feature].values\n",
    "        matrix = np.zeros((pred_tot,num_lags))\n",
    "        for i in range(num_lags):\n",
    "            matrix[:,i] = feature_vals[i:-num_lags-pred_length+i+1]\n",
    "        #ensure matrix is copy so that data isn't accidently altered\n",
    "        if(first):\n",
    "            data = matrix.copy()\n",
    "            first=False\n",
    "        else:\n",
    "            data = np.hstack((data,matrix))\n",
    "    return data,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_train_regression_dictionary(data_dictionary,ph,features=None):\n",
    "    \"\"\"\n",
    "    Creates a dictionary that maps patient number to both the train and test data, and train and test labels\n",
    "    Parameters:\n",
    "        data_dictionary(pandas dataframe): dictionary mapping patient id to both train and test data frames\n",
    "        ph(int): Prediction Horizon (time increments of 5 of how far to predict into the future)\n",
    "        features(list or None): columns of the pandas dataframe to use when creating the features. If None, use all columns\n",
    "    \"\"\"\n",
    "    #we only use the past 30 minutes to predict the next 30 minutes and 60 minutes\n",
    "    past_values = 6\n",
    "    train_dictionary = {}\n",
    "    for key in data_dictionary.keys():\n",
    "        train_data,train_labels = transform_to_regression(past_values,ph,data_dictionary[key][\"train\"],features)\n",
    "        test_data,test_labels = transform_to_regression(past_values,ph,data_dictionary[key][\"test\"],features)\n",
    "        train_dictionary[key] = {\"test_data\":test_data,\"test_labels\":test_labels,\"train_data\":train_data,\"train_labels\":train_labels}\n",
    "    return train_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Creates the train_test dictionary for each person, converts dataframes into regression data and labels, then stores in dictionary. Do this for 30 min horizon and 60 min horizon\n",
    "    Returns:\n",
    "        regression_dict_30: Dictionary containing regression data and labels for each patient for 30 min horizon\n",
    "        regression_dict_60: Dictionary containing regression data and labels for each patient for 60 min horizon\n",
    "    \"\"\"\n",
    "    people_dict= create_test_train_dictionary()\n",
    "    regression_dict_30 = create_test_train_regression_dictionary(people_dict,6)\n",
    "    regression_dict_60 = create_test_train_regression_dictionary(people_dict,12)\n",
    "\n",
    "    return regression_dict_30,regression_dict_60,people_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_results(models,data,ph,model_name):\n",
    "    \"\"\"Produces the dictionaries needed to create the results files\"\"\"\n",
    "    results_dict= {}\n",
    "    loop = tqdm(total=6,position=0)\n",
    "    \n",
    "    for key in models.keys():\n",
    "        model = models[key]\n",
    "        pred = model.predict(data[key][\"test_data\"])\n",
    "        met = get_metrics(data[key][\"test_labels\"][:,ph-1],pred[:,ph-1])\n",
    "        print(met)\n",
    "        df = pd.DataFrame(people_dict[key][\"test\"][\"glucose_level\"].iloc[11:])\n",
    "        print(len(df))\n",
    "        print(len(pred))\n",
    "        df[\"pred_glucose_level\"] = pred[:,ph-1]\n",
    "        print(\"df\",df)\n",
    "        results_dict[key] = [df,met[:2]]\n",
    "    create_results_files(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all(models,data,model_name,plot_path,clarke_path,metric_path,ph):\n",
    "    \"\"\"\n",
    "    Creates the predictions for each model\n",
    "    Parameters:\n",
    "        models(dictionary): dictionary containing the trained model for each person\n",
    "        data(dictionary): Dictionary containing the regression data for each person\n",
    "        model_name(string): name of model to include in file name\n",
    "        plot_path(str): Path to the directory where the plot for blood glucose will be stored\n",
    "        clarke_path(str): Path to the directory where the clarke error grid will be stored\n",
    "        metric_path(str): Path to the directory where the metric text file will be stored\n",
    "        ph(int):Prediction horizon being used (needs to be same as corresonding prediction horizon as the dictionary being used)\n",
    "    \n",
    "    Returns:\n",
    "        rmse: dictionary mapping patient number to RMSE for the specific ph\n",
    "        mae: dictionary mapping patient number to MAE for the specific ph\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics_dict = {}\n",
    "    #used to measure how long training and predicting will take for all 6 patients\n",
    "    loop = tqdm(total = 6,position=0)\n",
    "\n",
    "    #for each patient, make the predictions, then plot the results, save the results, and measure with the clarke error grid\n",
    "    for key in models.keys():\n",
    "        model = models[key]\n",
    "        pred = model.predict(data[key][\"test_data\"])\n",
    "        xvals = len(data[key][\"test_labels\"])\n",
    "        xvals = np.arange(xvals)\n",
    "        met = get_metrics(data[key][\"test_labels\"][:,ph-1],pred[:,ph-1])\n",
    "\n",
    "        file_name = str(key) + model_name + str(ph*5) + \".pdf\"\n",
    "        clarke_name = str(key) + model_name + str(ph*5) + \"clarke\" + \".pdf\"\n",
    "        \n",
    "        zones = clarke_error_grid(data[key][\"test_labels\"][:,ph-1],pred[:,ph-1],\"Patient \" + str(key) + \" \" + str(ph*5) + \" Minute Prediction\",clarke_path+clarke_name)\n",
    "        zones = tuple(zones)\n",
    "        met = met + zones\n",
    "        tot = np.sum(zones)\n",
    "        safe = (zones[0] + zones[1])/tot\n",
    "        unn = (zones[2])/tot\n",
    "        danger = (zones[3] + zones[4])/tot\n",
    "        met = met + (safe,unn,danger)\n",
    "        metrics_dict[key] = met\n",
    "        plot_title = \"Patient \" + str(key) + \" \" + str(ph*5) + \" Minute Prediction \" + model_name.capitalize() + \" Regression\"\n",
    "        comparison_plot(xvals,data[key][\"test_labels\"][:,ph-1],pred[:,ph-1],plot_title,plot_path+file_name)\n",
    "        loop.set_description(\"Finished iter in predict all\")\n",
    "        loop.update(1)\n",
    "    metric_name = model_name + str(ph*5) + \"metrics.txt\"\n",
    "    create_metrics_file(metric_path + metric_name,metrics_dict)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all_single(models,data,model_name,plot_path,clarke_path,metric_path,ph):\n",
    "    \"\"\"\n",
    "    Creates the predictions for each model. Similar to predict_all, but does not make the assumption that the predictions contain more than one value for each time step\n",
    "    Parameters:\n",
    "        models(dictionary): dictionary containing the trained model for each person\n",
    "        data(dictionary): Dictionary containing the regression data for each person\n",
    "        model_name(string): name of model to include in file name\n",
    "        plot_path(str): Path to the directory where the plot for blood glucose will be stored\n",
    "        clarke_path(str): Path to the directory where the clarke error grid will be stored\n",
    "        metric_path(str): Path to the directory where the metric text file will be stored\n",
    "        ph(int):Prediction horizon being used (needs to be same as corresonding prediction horizon as the dictionary being used)\n",
    "    \n",
    "    Returns:\n",
    "        rmse: dictionary mapping patient number to RMSE for the specific ph\n",
    "        mae: dictionary mapping patient number to MAE for the specific ph\n",
    "    \"\"\"\n",
    "    metrics_dict = {}\n",
    "    loop = tqdm(total = 6,position=0)\n",
    "\n",
    "    for key in models.keys():\n",
    "        model = models[key]\n",
    "        pred = model.predict(data[key][\"test_data\"])\n",
    "        xvals = len(data[key][\"test_labels\"])\n",
    "        xvals = np.arange(xvals)\n",
    "        met = get_metrics(data[key][\"test_labels\"][:,ph-1],pred)\n",
    "\n",
    "        file_name = str(key) + model_name + str(ph*5) + \".pdf\"\n",
    "        clarke_name = str(key) + model_name + str(ph*5) + \"clarke\" + \".pdf\"\n",
    "        \n",
    "        zones = clarke_error_grid(data[key][\"test_labels\"][:,ph-1],pred,\"Patient \" + str(key) + \" \" + str(ph*5) + \" Minute Prediction\",clarke_path+clarke_name)\n",
    "        zones = tuple(zones)\n",
    "        met = met + zones\n",
    "        metrics_dict[key] = met\n",
    "        comparison_plot(xvals,data[key][\"test_labels\"][:,ph-1],pred,str(key) + \" \" + model_name,plot_path+file_name)\n",
    "        loop.set_description(\"Finished iter in predict all\")\n",
    "        loop.update(1)\n",
    "    metric_name = model_name + str(ph*5) + \"metrics.txt\"\n",
    "    create_metrics_file(metric_path + metric_name,metrics_dict)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_files(results_dict30):#,results_dict60):\n",
    "    \"\"\"\n",
    "    Writes the needed files for the competition results.\n",
    "    The files are:\n",
    "        results.txt: This is a text file that contains the results for the RMSE and MAE of all the patients. This file will need to be converted to a pdf\n",
    "        \n",
    "    Parameters:\n",
    "            results_dict30: Dictionary that maps each person id to an iterable (list or tuple) containing the following structures\n",
    "                results[0]: Contains a pandas dataframe with 3 columns. First column is the time value, the second column is the actual blood glucose values, and the third column is the predicted blood glucose values for the 30 minute prediction horizon\n",
    "                results[1]: Is an iterable (list or tuple) with the following values in order: RMSE,MAE\n",
    "                \n",
    "            results_dict60: Dictionary that maps each person id to an iterable (list or tuple) containing the following structures\n",
    "                results[0]: Contains a pandas dataframe with 3 columns. First column is the time value, the second column is the actual blood glucose values, and the third column is the predicted blood glucose values for the 60 minute prediction horizon\n",
    "                results[1]: Is an iterable (list or tuple) with the following values in order: RMSE,MAE\n",
    "    \"\"\"\n",
    "    patients = [540,544,552,567,584,596]\n",
    "    with open(\"results.txt\",\"w+\") as results:\n",
    "        results.write(\"30 Minute Prediction Horizon\" +\"\\n\" + \"\\n\")\n",
    "        rmse = 0\n",
    "        mae = 0\n",
    "        for patient in patients:\n",
    "            file_name = \"lasso \" + str(patient) + \" \" + \"30.txt\"\n",
    "            results_dict30[patient][0].to_csv(file_name,sep=\" \")\n",
    "            results.write(\"Patient \" + str(patient) + \"\\n\")\n",
    "            results.write(\"RMSE: \" + str(results_dict30[patient][1][0]) + \"\\n\")\n",
    "            results.write(\"MAE: \" + str(results_dict30[patient][1][1]) + \"\\n\" + \"\\n\")\n",
    "            rmse += results_dict30[patient][1][0]\n",
    "            mae += results_dict30[patient][1][1]\n",
    "        rmse = rmse / 6\n",
    "        mae = mae / 6\n",
    "        results.write(\"Average RMSE: \" + str(rmse) + \"\\n\")\n",
    "        results.write(\"Average MAE: \" + str(mae) + \"\\n\" + \"\\n\")\n",
    "        \n",
    "        \"\"\"for patient in patients:\n",
    "            file_name = \"lasso \" + str(patient) + \" \" + \"60.txt\"    \n",
    "            np.savetxt(file_name,results_dict60[patient][0].values,delimiter = \" \")\n",
    "            results.write(\"Patient \" + str(patient) + \"\\n\")\n",
    "            results.write(\"RMSE: \" + results_dict60[patient][1][0] + \"\\n\")\n",
    "            results.write(\"MAE: \" + results_dict60[patient][1][1] + \"\\n\" + \"\\n\")\n",
    "            rmse += results_dict60[patient][1][0]\n",
    "            mae += results_dict60[patient][1][1]\n",
    "        rmse = rmse / 6\n",
    "        mae = mae / 6\n",
    "        results.write(\"Average RMSE: \" + rmse + \"\\n\")\n",
    "        results.write(\"Average MAE: \" + mae + \"\\n\" + \"\\n\")\"\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create all needed directories\n",
    "create_all_directories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the global dictionaries to be used in training\n",
    "regression_dict_30,regression_dict_60,people_dict = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_glucose():\n",
    "    \"\"\"\n",
    "    Calculates the average blood glucose level of the test set for each patient\n",
    "    \"\"\"\n",
    "    patients = [540,544,552, 567, 584, 596]\n",
    "    for key in patients:\n",
    "        data = people_dict[key][\"test\"][\"glucose_level\"].values\n",
    "        print(\"Patient \" + str(key),\"Average glucose level\",np.mean(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_glucose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine Regression\n",
    "## RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rbf_models(data,ph):\n",
    "    \"\"\"\n",
    "    Train the Support Vector Machine with RBF Kernel for each person and saves them to pickle files\n",
    "    Parameters:\n",
    "        data(dictionary): Dictionary containing the regression information for each patient\n",
    "        ph(6 or 12): The length of the prediction horizon. 6 corresponds to 30-minutes and 12 corresponds to 60-minutes\n",
    "    \"\"\"\n",
    "    path = \"pickle/\" + str(ph*5) + \"min/svm/rbf/\"\n",
    "    for key in data.keys():\n",
    "        rbf = SVR()\n",
    "        rbf = MultiOutputRegressor(rbf)\n",
    "        rbf.fit(data[key][\"train_data\"],data[key][\"train_labels\"])\n",
    "        name = str(key) + \"rbf\" + str(ph*5)+ \".pickle\"\n",
    "        pickle.dump(rbf,open(path+name,'wb'))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rbf_models(data,ph):\n",
    "    \"\"\"\n",
    "    Load each trained Support Vector Machine with RBF Kernel from the correct pickle file, then predict on test data for each model\n",
    "    \n",
    "    Parameters:\n",
    "        data(dictionary): Dictionary containing the regression information for each patient\n",
    "        ph(6 or 12): The length of the prediction horizon. 6 corresponds to 30-minutes and 12 corresponds to 60-minutes\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    path = \"pickle/\" + str(ph*5) + \"min/svm/rbf/\"\n",
    "    file_path = \"plots/\" + str(ph*5) + \"min/svm/rbf/\"\n",
    "    clarke_path = \"clarke/\" + str(ph*5) + \"min/svm/rbf/\"\n",
    "    metric_path =  \"metrics/\" + str(ph*5) + \"min/svm/rbf/\"    \n",
    "    for key in data.keys():\n",
    "        name = str(key) + \"rbf\" + str(ph*5)+ \".pickle\"\n",
    "        models[key] = pickle.load(open(path+name,'rb'))\n",
    "    predict_all(models,data,\"rbf\",file_path,clarke_path,metric_path,ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rbf_models(regression_dict_30,6)\n",
    "train_rbf_models(regression_dict_60,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_rbf_models(regression_dict_30,6)\n",
    "predict_rbf_models(regression_dict_60,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_hard_filter_rbf_models(regression_dict_30,6,4)\n",
    "predict_hard_filter_rbf_models(regression_dict_60,12,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_poly_models(data,ph):\n",
    "    \"\"\"\n",
    "    Train the Support Vector Machine with Polynomial Kernel for each person and saves them to pickle files\n",
    "    \n",
    "    Parameters:\n",
    "        data(dictionary): Dictionary containing the regression information for each patient\n",
    "        ph(6 or 12): The length of the prediction horizon. 6 corresponds to 30-minutes and 12 corresponds to 60-minutes\n",
    "    \"\"\"\n",
    "    loop = tqdm(total = 6,position=0)\n",
    "\n",
    "    path = \"pickle/\" + str(ph*5) + \"min/svm/poly/\"\n",
    "    for key in data.keys():\n",
    "        poly = SVR(kernel=\"poly\")\n",
    "        poly = MultiOutputRegressor(poly)\n",
    "        poly.fit(data[key][\"train_data\"],data[key][\"train_labels\"])\n",
    "        name = str(key) + \"poly\" + str(ph*5)+ \".pickle\"\n",
    "        pickle.dump(poly,open(path+name,'wb'))\n",
    "        loop.set_description(\"Time has finished\")\n",
    "        loop.update(1)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_poly_models(data,ph):\n",
    "    \"\"\"\n",
    "    Load each trained Support Vector Machine with Pollynomial Kernel from the correct pickle file, then predict on test data for each model\n",
    "    \n",
    "    Parameters:\n",
    "        data(dictionary): Dictionary containing the regression information for each patient\n",
    "        ph(6 or 12): The length of the prediction horizon. 6 corresponds to 30-minutes and 12 corresponds to 60-minutes\n",
    "    \"\"\"\n",
    "    path = \"pickle/\" + str(ph*5) + \"min/svm/poly/\"\n",
    "    file_path = \"plots/\" + str(ph*5) + \"min/svm/poly/\"\n",
    "    clarke_path = \"clarke/\" + str(ph*5) + \"min/svm/poly/\"\n",
    "    metric_path =  \"metrics/\" + str(ph*5) + \"min/svm/poly/\"    \n",
    "    models = {}\n",
    "    for key in data.keys():\n",
    "        name = str(key) + \"poly\" + str(ph*5) + \".pickle\"\n",
    "        models[key] = pickle.load(open(path+name,'rb'))\n",
    "    predict_all(models,data,\"poly\",file_path,clarke_path,metric_path,ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_poly_models(regression_dict_30,6)\n",
    "train_poly_models(regression_dict_60,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_poly_models(regression_dict_30,6)\n",
    "predict_poly_models(regression_dict_60,12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sigmoid_models(data,ph):\n",
    "    \"\"\"\n",
    "    Train the Support Vector Machine with Sigmoid Kernel for each person and saves them to pickle files\n",
    "    \n",
    "    Parameters:\n",
    "        data(dictionary): Dictionary containing the regression information for each patient\n",
    "        ph(6 or 12): The length of the prediction horizon. 6 corresponds to 30-minutes and 12 corresponds to 60-minutes\n",
    "    \"\"\"\n",
    "    loop = tqdm(total = 6,position=0)\n",
    "\n",
    "    path = \"pickle/\" + str(ph*5) + \"min/svm/sigmoid/\"\n",
    "    for key in data.keys():\n",
    "        sig = SVR(kernel=\"sigmoid\")\n",
    "        sig = MultiOutputRegressor(sig)\n",
    "        sig.fit(data[key][\"train_data\"],data[key][\"train_labels\"])\n",
    "        name = str(key) + \"sig\" + str(ph*5)+ \".pickle\"\n",
    "        pickle.dump(sig,open(path+name,'wb'))\n",
    "        loop.set_description(\"Time has finished\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sigmoid_models(data,ph):\n",
    "    \"\"\"\n",
    "    Load each trained Support Vector Machine with Sigmoid Kernel from the correct pickle file, then predict on test data for each model\n",
    "    \n",
    "    Parameters:\n",
    "        data(dictionary): Dictionary containing the regression information for each patient\n",
    "        ph(6 or 12): The length of the prediction horizon. 6 corresponds to 30-minutes and 12 corresponds to 60-minutes\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    path = \"pickle/\" + str(ph*5) + \"min/svm/sigmoid/\"\n",
    "    file_path = \"plots/\" + str(ph*5) + \"min/svm/sigmoid/\"\n",
    "    clarke_path = \"clarke/\" + str(ph*5) + \"min/svm/sigmoid/\"\n",
    "    metric_path =  \"metrics/\" + str(ph*5) + \"min/svm/sigmoid/\"    \n",
    "    for key in data.keys():\n",
    "        name = str(key) + \"sig\" + str(ph*5)+ \".pickle\"\n",
    "        models[key] = pickle.load(open(path+name,'rb'))\n",
    "    predict_all(models,data,\"sigmoid\",file_path,clarke_path,metric_path,ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sigmoid_models(regression_dict_30,6)\n",
    "train_sigmoid_models(regression_dict_60,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sigmoid_models(regression_dict_30,6)\n",
    "predict_sigmoid_models(regression_dict_60,12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_models(data,ph):\n",
    "    \"\"\"\n",
    "    Train the Linear Regression Model with no regularization for each person and saves them to pickle files\n",
    "    \n",
    "    Parameters:\n",
    "        data(dictionary): Dictionary containing the regression information for each patient\n",
    "        ph(6 or 12): The length of the prediction horizon. 6 corresponds to 30-minutes and 12 corresponds to 60-minutes\n",
    "    \"\"\"\n",
    "    path = \"pickle/\" + str(ph*5) + \"min/linear/\"\n",
    "    for key in data.keys():\n",
    "        lr = linear_model.LinearRegression(n_jobs=-1)\n",
    "        lr = MultiOutputRegressor(lr)\n",
    "        lr.fit(data[key][\"train_data\"],data[key][\"train_labels\"])\n",
    "        name = str(key) + \"lr\" + str(ph*5)+ \".pickle\"\n",
    "        pickle.dump(lr,open(path+name,'wb'))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_linear_models(data,ph):\n",
    "    \"\"\"\n",
    "    Load each trained Linear Regression model with no regularization from the correct pickle file, then predict on test data for each model\n",
    "    \n",
    "    Parameters:\n",
    "        data(dictionary): Dictionary containing the regression information for each patient\n",
    "        ph(6 or 12): The length of the prediction horizon. 6 corresponds to 30-minutes and 12 corresponds to 60-minutes\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    path = \"pickle/\" + str(ph*5) + \"min/linear/\"\n",
    "    file_path = \"plots/\" + str(ph*5) + \"min/linear/\"\n",
    "    clarke_path = \"clarke/\" + str(ph*5) + \"min/linear/\"\n",
    "    metric_path =  \"metrics/\" + str(ph*5) + \"min/linear/\"\n",
    "    for key in data.keys():\n",
    "        name = str(key) + \"lr\" + str(ph*5)+ \".pickle\"\n",
    "        models[key] = pickle.load(open(path+name,'rb'))\n",
    "    predict_all(models,data,\"lr\",file_path,clarke_path,metric_path,ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_linear_models(regression_dict_30,6)\n",
    "train_linear_models(regression_dict_60,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_linear_models(regression_dict_30,6)\n",
    "predict_linear_models(regression_dict_60,12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ridge_models(data,ph):\n",
    "    \"\"\"\n",
    "    Train the Linear Regression Model with Ridge Regularization for each person and saves them to pickle files\n",
    "    Parameters:\n",
    "        data(dictionary): Dictionary containing the regression information for each patient\n",
    "        ph(6 or 12): The length of the prediction horizon. 6 corresponds to 30-minutes and 12 corresponds to 60-minutes\n",
    "    \"\"\"\n",
    "    path = \"pickle/\" + str(ph*5) + \"min/ridge/\"\n",
    "    for key in data.keys():\n",
    "        lr = linear_model.Ridge()\n",
    "        lr = MultiOutputRegressor(lr)\n",
    "        lr.fit(data[key][\"train_data\"],data[key][\"train_labels\"])\n",
    "        name = str(key) + \"ridge\" + str(ph*5)+ \".pickle\"\n",
    "        pickle.dump(lr,open(path+name,'wb'))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ridge_models(data,ph):\n",
    "    \"\"\"\n",
    "    Load each trained Linear Regression Model with Ridge Regularization from the correct pickle file, then predict on test data for each model\n",
    "    \n",
    "    Parameters:\n",
    "        data(dictionary): Dictionary containing the regression information for each patient\n",
    "        ph(6 or 12): The length of the prediction horizon. 6 corresponds to 30-minutes and 12 corresponds to 60-minutes\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    path = \"pickle/\" + str(ph*5) + \"min/ridge/\"\n",
    "    file_path = \"plots/\" + str(ph*5) + \"min/ridge/\"\n",
    "    clarke_path = \"clarke/\" + str(ph*5) + \"min/ridge/\"\n",
    "    metric_path =  \"metrics/\" + str(ph*5) + \"min/ridge/\"\n",
    "    for key in data.keys():\n",
    "        name = str(key) + \"ridge\" + str(ph*5)+ \".pickle\"\n",
    "        models[key] = pickle.load(open(path+name,'rb'))\n",
    "    predict_all(models,data,\"ridge\",file_path,clarke_path,metric_path,ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ridge_models(regression_dict_30,6)\n",
    "train_ridge_models(regression_dict_60,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ridge_models(regression_dict_30,6)\n",
    "predict_ridge_models(regression_dict_60,12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lasso_models(data,ph):\n",
    "    \"\"\"\n",
    "    Train the Linear Regression Model with Lasso Regularization for each person and saves them to pickle files\n",
    "    Parameters:\n",
    "        data(dictionary): Dictionary containing the regression information for each patient\n",
    "        ph(6 or 12): The length of the prediction horizon. 6 corresponds to 30-minutes and 12 corresponds to 60-minutes\n",
    "    \"\"\"\n",
    "    path = \"pickle/\" + str(ph*5) + \"min/lasso/\"\n",
    "    loop = tqdm(total = 6,position=0)\n",
    "\n",
    "    for key in data.keys():\n",
    "        lr = linear_model.Lasso(max_iter=10000)\n",
    "        lr = MultiOutputRegressor(lr)\n",
    "        lr.fit(data[key][\"train_data\"],data[key][\"train_labels\"])\n",
    "        name = str(key) + \"lasso\" + str(ph*5)+ \".pickle\"\n",
    "        pickle.dump(lr,open(path+name,'wb'))\n",
    "        loop.set_description(\"Done\")\n",
    "        loop.update(1)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lasso_models(data,ph):\n",
    "    \"\"\"\n",
    "    Load each trained Linear Model with Lasso Regularization from the correct pickle file, then predict on test data for each model\n",
    "    \n",
    "    Parameters:\n",
    "        data(dictionary): Dictionary containing the regression information for each patient\n",
    "        ph(6 or 12): The length of the prediction horizon. 6 corresponds to 30-minutes and 12 corresponds to 60-minutes\n",
    "    \"\"\"\n",
    "    path = \"pickle/\" + str(ph*5) + \"min/lasso/\"\n",
    "    file_path = \"plots/\" + str(ph*5) + \"min/lasso/\"\n",
    "    clarke_path = \"clarke/\" + str(ph*5) + \"min/lasso/\"\n",
    "    metric_path =  \"metrics/\" + str(ph*5) + \"min/lasso/\"    \n",
    "    models = {}\n",
    "    \n",
    "    for key in data.keys():\n",
    "        name = str(key) + \"lasso\" + str(ph*5)+ \".pickle\"\n",
    "        models[key] = pickle.load(open(path+name,'rb'))\n",
    "    predict_all(models,data,\"lasso\",file_path,clarke_path,metric_path,ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lasso_models(regression_dict_30,6)\n",
    "train_lasso_models(regression_dict_60,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_lasso_models(regression_dict_30,6)\n",
    "predict_lasso_models(regression_dict_60,12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Net Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_elasticnet_models(data,ph):\n",
    "    \"\"\"\n",
    "    Train the Linear Regression Model with Elastic Net Regularization for each person and saves them to pickle files\n",
    "    Parameters:\n",
    "        data(dictionary): Dictionary containing the regression information for each patient\n",
    "    \"\"\"\n",
    "    path = \"pickle/\" + str(ph*5) + \"min/elastic/\"\n",
    "    loop = tqdm(total = 6,position=0)\n",
    "    for key in data.keys():\n",
    "        lr = linear_model.ElasticNet(max_iter=10000)\n",
    "        lr = MultiOutputRegressor(lr)\n",
    "        lr.fit(data[key][\"train_data\"],data[key][\"train_labels\"])\n",
    "        name = str(key) + \"elasticnet\" + str(ph*5)+ \".pickle\"\n",
    "        pickle.dump(lr,open(path+name,'wb'))\n",
    "        loop.set_description(\"Done\")\n",
    "        loop.update(1)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_elasticnet_models(data,ph):\n",
    "    \"\"\"\n",
    "    Load each trained Linear Model with Elastic Net Regularization from the correct pickle file, then predict on test data for each model\n",
    "    \n",
    "    Parameters:\n",
    "        data(dictionary): Dictionary containing the regression information for each patient\n",
    "        ph(6 or 12): The length of the prediction horizon. 6 corresponds to 30-minutes and 12 corresponds to 60-minutes\n",
    "    \"\"\"\n",
    "    path = \"pickle/\" + str(ph*5) + \"min/elastic/\"\n",
    "    file_path = \"plots/\" + str(ph*5) + \"min/elastic/\"\n",
    "    clarke_path = \"clarke/\" + str(ph*5) + \"min/elastic/\"\n",
    "    metric_path =  \"metrics/\" + str(ph*5) + \"min/elastic/\"    \n",
    "    models = {}\n",
    "    for key in data.keys():\n",
    "        name = str(key) + \"elasticnet\" + str(ph*5)+ \".pickle\"\n",
    "        models[key] = pickle.load(open(path+name,'rb'))\n",
    "    predict_all(models,data,\"elasticnet\",file_path,clarke_path,metric_path,ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_elasticnet_models(regression_dict_30,6)\n",
    "train_elasticnet_models(regression_dict_60,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_elasticnet_models(regression_dict_30,6)\n",
    "predict_elasticnet_models(regression_dict_60,12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_knn_models(data,ph):\n",
    "    \"\"\"\n",
    "    Train the K Nearest Neighbors Regressor for each person and saves them to pickle files\n",
    "    Parameters:\n",
    "        data(dictionary): Dictionary containing the regression information for each patient\n",
    "        ph(6 or 12): The length of the prediction horizon. 6 corresponds to 30-minutes and 12 corresponds to 60-minutes\n",
    "    \"\"\"\n",
    "    path = \"pickle/\" + str(ph*5) + \"min/knn/\"\n",
    "    for key in data.keys():\n",
    "        knn = KNeighborsRegressor(n_jobs=-1)\n",
    "        knn = MultiOutputRegressor(knn)\n",
    "        knn.fit(data[key][\"train_data\"],data[key][\"train_labels\"])\n",
    "        name = str(key) + \"knn\" + str(ph*5)+ \".pickle\"\n",
    "        pickle.dump(knn,open(path+name,'wb'))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_knn_models(data,ph):\n",
    "    \"\"\"\n",
    "    Load each K Nearest Neighbors Regressor from the correct pickle file, then predict on test data for each model\n",
    "    \n",
    "    Parameters:\n",
    "        data(dictionary): Dictionary containing the regression information for each patient\n",
    "        ph(6 or 12): The length of the prediction horizon. 6 corresponds to 30-minutes and 12 corresponds to 60-minutes\n",
    "    \"\"\"\n",
    "    path = \"pickle/\" + str(ph*5) + \"min/knn/\"\n",
    "    file_path = \"plots/\" + str(ph*5) + \"min/knn/\"\n",
    "    clarke_path = \"clarke/\" + str(ph*5) + \"min/knn/\"\n",
    "    metric_path =  \"metrics/\" + str(ph*5) + \"min/knn/\"    \n",
    "    models = {}\n",
    "    for key in data.keys():\n",
    "        name = str(key) + \"knn\" + str(ph*5)+ \".pickle\"\n",
    "        models[key] = pickle.load(open(path+name,'rb'))\n",
    "    predict_all(models,data,\"knn\",file_path,clarke_path,metric_path,ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_knn_models(regression_dict_30,6)\n",
    "train_knn_models(regression_dict_60,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_knn_models(regression_dict_30,6)\n",
    "predict_knn_models(regression_dict_60,12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf_models(data,ph):\n",
    "    \"\"\"\n",
    "    Train the Random Forest for each person and saves them to pickle files\n",
    "    Parameters:\n",
    "        data(dictionary): Dictionary containing the regression information for each patient\n",
    "        ph(6 or 12): The length of the prediction horizon. 6 corresponds to 30-minutes and 12 corresponds to 60-minutes\n",
    "    \"\"\"\n",
    "    path = \"pickle/\" + str(ph*5) + \"min/rf/\"\n",
    "    loop = tqdm(total = 6,position=0)\n",
    "\n",
    "    for key in data.keys():\n",
    "        rf = RandomForestRegressor(max_depth = 4)\n",
    "        rf = MultiOutputRegressor(rf)\n",
    "        rf.fit(data[key][\"train_data\"],data[key][\"train_labels\"])\n",
    "        name = str(key) + \"rf\" + str(ph*5)+ \".pickle\"\n",
    "        pickle.dump(rf,open(path+name,'wb'))\n",
    "        loop.update(1)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rf_models(data,ph):\n",
    "    \"\"\"\n",
    "    Load each Random Forest from the correct pickle file, then predict on test data for each model\n",
    "    \n",
    "    Parameters:\n",
    "        data(dictionary): Dictionary containing the regression information for each patient\n",
    "        ph(6 or 12): The length of the prediction horizon. 6 corresponds to 30-minutes and 12 corresponds to 60-minutes\n",
    "    \"\"\"\n",
    "    path = \"pickle/\" + str(ph*5) + \"min/rf/\"\n",
    "    file_path = \"plots/\" + str(ph*5) + \"min/rf/\"\n",
    "    clarke_path = \"clarke/\" + str(ph*5) + \"min/rf/\"\n",
    "    metric_path =  \"metrics/\" + str(ph*5) + \"min/rf/\"    \n",
    "    models = {}\n",
    "    for key in data.keys():\n",
    "        name = str(key) + \"rf\" + str(ph*5)+ \".pickle\"\n",
    "        models[key] = pickle.load(open(path+name,'rb'))\n",
    "    predict_all(models,data,\"rf \",file_path,clarke_path,metric_path,ph)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rf_models(regression_dict_30,6)\n",
    "train_rf_models(regression_dict_60,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_rf_models(regression_dict_30,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_rf_models(regression_dict_60,12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gradient_models(data,ph):\n",
    "    \"\"\"\n",
    "    Train Gradient Boosted Trees for each person and saves them to pickle files\n",
    "    Parameters:\n",
    "        data(dictionary): Dictionary containing the regression information for each patient\n",
    "        ph(6 or 12): The length of the prediction horizon. 6 corresponds to 30-minutes and 12 corresponds to 60-minutes\n",
    "    \"\"\"\n",
    "    path = \"pickle/\" + str(ph*5) + \"min/gradient/\"\n",
    "    loop = tqdm(total = 6,position=0)\n",
    "\n",
    "    for key in data.keys():\n",
    "        grad = GradientBoostingRegressor()\n",
    "        grad = MultiOutputRegressor(grad)\n",
    "        grad.fit(data[key][\"train_data\"],data[key][\"train_labels\"])\n",
    "        name = str(key) + \"gradient\" + str(ph*5)+ \".pickle\"\n",
    "        pickle.dump(grad,open(path+name,'wb'))\n",
    "        loop.update(1)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gradient_models(data,ph):\n",
    "    \"\"\"\n",
    "    Load each trained Gradient Boosted Trees from the correct pickle file, then predict on test data for each model\n",
    "    \n",
    "    Parameters:\n",
    "        data(dictionary): Dictionary containing the regression information for each patient\n",
    "        ph(6 or 12): The length of the prediction horizon. 6 corresponds to 30-minutes and 12 corresponds to 60-minutes\n",
    "    \"\"\"\n",
    "    path = \"pickle/\" + str(ph*5) + \"min/gradient/\"\n",
    "    models = {}\n",
    "    for key in data.keys():\n",
    "        name = str(key) + \"gradient\" + str(ph*5)+ \".pickle\"\n",
    "        models[key] = pickle.load(open(path+name,'rb'))\n",
    "    file_path = \"plots/\" + str(ph*5) + \"min/gradient/\"\n",
    "    clarke_path = \"clarke/\" + str(ph*5) + \"min/gradient/\"\n",
    "    metric_path =  \"metrics/\" + str(ph*5) + \"min/gradient/\"    \n",
    "    predict_all(models,data,\"gradient\",file_path,clarke_path,metric_path,ph)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gradient_models(regression_dict_30,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_gradient_models(regression_dict_30,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gradient_models(regression_dict_60,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_gradient_models(regression_dict_60,12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgb_models(data,ph):\n",
    "    \"\"\"\n",
    "    Train Extreme Gradient Boosted Trees Regressor for each person and saves them to pickle files\n",
    "    Parameters:\n",
    "        data(dictionary): Dictionary containing the regression information for each patient\n",
    "        ph(int): length of prediction horizon (take time frame and divide by 5) ie: 30min prediction = 6\n",
    "    \"\"\"\n",
    "    path = \"pickle/\" + str(ph*5) + \"min/xgb/\"\n",
    "    for key in data.keys():\n",
    "        xgb = XGBRegressor(max_depth=4,n_jobs=-1,learning_rate=1)\n",
    "        xgb = MultiOutputRegressor(xgb)\n",
    "        param_grid = {\"estimator__alpha\":[0,0.5,1],\"estimator__gamma\":[0,0.5,1], \"estimator__lambda\":[1,2,3]}\n",
    "        xgb_gs = GridSearchCV(xgb,param_grid,cv=5,refit=True,verbose=1,n_jobs=-1).fit(data[key][\"train_data\"],data[key][\"train_labels\"])\n",
    "        best = xgb_gs.best_estimator_\n",
    "        name = str(key) + \"xgb\" + str(ph*5)+ \".pickle\"\n",
    "        pickle.dump(best,open(path+name,'wb'))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_xgb_models(data,ph):\n",
    "    \"\"\"\n",
    "    Load each trained Extreme Gradient Boosted Trees from the correct pickle file, then predict on test data for each model\n",
    "    \n",
    "    Parameters:\n",
    "        data(dictionary): Dictionary containing the regression information for each patient\n",
    "        ph(6 or 12): The length of the prediction horizon. 6 corresponds to 30-minutes and 12 corresponds to 60-minutes\n",
    "    \"\"\"\n",
    "    path = \"pickle/\" + str(ph*5) + \"min/xgb/\"\n",
    "    models = {}\n",
    "    for key in data.keys():\n",
    "        name = str(key) + \"xgb\" + str(ph*5)+ \".pickle\"\n",
    "        models[key] = pickle.load(open(path+name,'rb'))\n",
    "    file_path = \"plots/\" + str(ph*5) + \"min/xgb/\"\n",
    "    clarke_path = \"clarke/\" + str(ph*5) + \"min/xgb/\"\n",
    "    metric_path =  \"metrics/\" + str(ph*5) + \"min/xgb/\"    \n",
    "    predict_all(models,data,\"xgb\",file_path,clarke_path,metric_path,ph)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xgb_models(regression_dict_30,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_xgb_models(regression_dict_30,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xgb_models(regression_dict_60,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_xgb_models(regression_dict_60,12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp_models(data,ph):\n",
    "    \"\"\"\n",
    "    Train Extreme Gradient Boosted Trees Regressor for each person and saves them to pickle files\n",
    "    Parameters:\n",
    "        data(dictionary): Dictionary containing the regression information for each patient\n",
    "        ph(int): length of prediction horizon (take time frame and divide by 5) ie: 30min prediction = 6\n",
    "    \"\"\"\n",
    "    path = \"pickle/\" + str(ph*5) + \"min/mlp/\"\n",
    "    loop = tqdm(total = 6,position=0)\n",
    "    ndim = 6*19\n",
    "    for key in data.keys():\n",
    "        mlp = MLPRegressor(hidden_layer_sizes=(100,100,100),solver=\"adam\",max_iter=1000)\n",
    "        mlp = MultiOutputRegressor(mlp)\n",
    "        mlp.fit(data[key][\"train_data\"],data[key][\"train_labels\"])\n",
    "        name = str(key) + \"mlp\" + str(ph*5)+ \".pickle\"\n",
    "        pickle.dump(mlp,open(path+name,'wb'))\n",
    "        loop.update(1)\n",
    "    return\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mlp_models(data,ph):\n",
    "    \"\"\"\n",
    "    Load each MLPRegressor from the correct pickle file, then predict on test data for each model\n",
    "    \n",
    "    Parameters:\n",
    "        data(dictionary): Dictionary containing the regression information for each patient\n",
    "        ph(6 or 12): The length of the prediction horizon. 6 corresponds to 30-minutes and 12 corresponds to 60-minutes\n",
    "    \"\"\"\n",
    "    path = \"pickle/\" + str(ph*5) + \"min/mlp/\"\n",
    "    file_path = \"plots/\" + str(ph*5) + \"min/mlp/\"\n",
    "    clarke_path = \"clarke/\" + str(ph*5) + \"min/mlp/\"\n",
    "    metric_path =  \"metrics/\" + str(ph*5) + \"min/mlp/\"    \n",
    "    models = {}\n",
    "    for key in data.keys():\n",
    "        name = str(key) + \"mlp\" + str(ph*5)+ \".pickle\"\n",
    "        models[key] = pickle.load(open(path+name,'rb'))\n",
    "    predict_all(models,data,\"mlp\",file_path,clarke_path,metric_path,ph)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mlp_models(regression_dict_30,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_mlp_models(regression_dict_30,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mlp_models(regression_dict_60,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_mlp_models(regression_dict_60,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we will include a comment about how a heavily engineered package like facebook's prophet can do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df540 = parse_XML(\"OHIODATA/OhioT1DM-2-training/540-ws-training.xml\")\n",
    "temp = pd.DataFrame(np.array([df540[\"glucose_level\"].index,df540[\"glucose_level\"].values]).T, columns = [\"ds\",\"y\"])\n",
    "temp = temp.iloc[-18:-6]   #using the entire dataset just puts it around the average. This is the default linear method, it would be better to create a function for a different loss function as described in the documentation.\n",
    "\n",
    "m = Prophet()\n",
    "m = Prophet(changepoint_prior_scale=0.01).fit(temp)\n",
    "future = m.make_future_dataframe(periods=1, freq='H')\n",
    "fcst = m.predict(future)\n",
    "fig = m.plot(fcst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalman Filters\n",
    "\n",
    "https://stackoverflow.com/questions/43377626/how-to-use-kalman-filter-in-python-for-location-data\n",
    "\n",
    "https://github.com/rlabbe/filterpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kalman Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REGULAR KALMAN FILTER\n",
    "def EKF(data, window = 6):\n",
    "    my_filter = KalmanFilter(dim_x=2, dim_z=1)\n",
    "    my_filter.x = np.array([[data[0]],[-0.9975063]])           # initial state (location and velocity)\n",
    "    my_filter.F = np.array([[1.,1.],[0.,1.]])                  # state transition matrix\n",
    "    my_filter.H = np.array([[1.,0.]])                          # Measurement function\n",
    "    my_filter.P = np.array([[1000.,    0.], [   0., 1000.] ])  # covariance matrix\n",
    "    my_filter.R = 5                                            # state uncertainty\n",
    "    my_filter.Q = Q_discrete_white_noise(2, 0.1, .13)          # process uncertainty\n",
    "\n",
    "    pred1 = []\n",
    "    for i in range(1,len(data)):\n",
    "        z = data[i]\n",
    "        temp = my_filter\n",
    "        for _ in range(window):         \n",
    "            temp.predict()\n",
    "        pred1.append(temp.x[0][0])\n",
    "        my_filter.update(z)          #update with new data\n",
    "    return pred1\n",
    "    \n",
    "dataukf = df540[\"glucose_level\"].values\n",
    "pred1 = EKF(dataukf, window = 12)\n",
    "\n",
    "\n",
    "avgs = np.zeros(12)\n",
    "for per in dfs_test:\n",
    "    datakf = per[\"glucose_level\"].values\n",
    "    pred1 = EKF(datakf, window = 6)\n",
    "    mets = get_metrics(datakf[206:], np.array(pred1[200:-5]))\n",
    "    print()\n",
    "    print(\"Person:\")\n",
    "    for i in range(len(mets)):\n",
    "        avgs[i] += mets[i]\n",
    "        print(labels[i] + \" : \" + str(mets[i]))\n",
    "print(avgs/6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unscented Kalman Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unscented Kalman Filter\n",
    "def UKF(window = 6):\n",
    "    def fx(x, dt):\n",
    "        # state transition function - predict next state based\n",
    "        # on constant velocity model x = vt + x_0\n",
    "        F = np.array([[48, 8, 0, 0],\n",
    "                      [0, 48, 4, 0],\n",
    "                      [0, 0, 48, 8],\n",
    "                      [0, 0, 0, 48]], dtype=float)\n",
    "        return np.dot(F, x)\n",
    "\n",
    "    def hx(x):\n",
    "       # measurement function - convert state into a measurement\n",
    "       # where measurements are [x_pos, y_pos]\n",
    "       return np.array([x[0], x[2]])\n",
    "\n",
    "    dt = 12\n",
    "    # create sigma points to use in the filter. This is standard for Gaussian processes\n",
    "    points = MerweScaledSigmaPoints(4, alpha=.1, beta=2., kappa=-1)\n",
    "\n",
    "    kfU = UnscentedKalmanFilter(dim_x=4, dim_z=2, dt=dt, fx=fx, hx=hx, points=points)\n",
    "    kfU.x = np.array([measurements[0][0], measurements[0][1], 0., 0.]) # initial state\n",
    "    kfU.P *= 0.1 # initial uncertainty\n",
    "    z_std = 0\n",
    "    kfU.R = np.diag([z_std**2, z_std**2]) # 1 standard\n",
    "    kfU.Q = Q_discrete_white_noise(dim=2, dt=dt, var=0.01**2, block_size=2)\n",
    "\n",
    "    predict = []\n",
    "    for z in measurements:\n",
    "        kfU.predict(dt = 6)\n",
    "        predict.append(-1*kfU.x[2]/2270)\n",
    "        kfU.update(z)\n",
    "    return predict\n",
    "\n",
    "dataukf = df540[\"glucose_level\"].values\n",
    "measurements = zip(range(len(dataukf)),dataukf)\n",
    "measurements = np.array(tuple(measurements))\n",
    "pred2 = UKF()\n",
    "plot_compare(dataukf, pred2, 200, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgs = np.zeros(12)\n",
    "for per in dfs_test:\n",
    "    dataukf = per[\"glucose_level\"].values\n",
    "    measurements = zip(range(len(dataukf)),dataukf)\n",
    "    measurements = np.array(tuple(measurements))\n",
    "    pred2 = UKF(window = 6)\n",
    "    mets = get_metrics(dataukf[106:], np.array(pred2[100:-6]))\n",
    "    print()\n",
    "    print(\"Person:\")\n",
    "    for i in range(len(mets)):\n",
    "        avgs[i] += mets[i]\n",
    "        print(labels[i] + \" : \" + str(mets[i]))\n",
    "print(avgs/6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARMA (Autoregressive Moving Average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df540 = parse_XML(\"OHIODATA/OhioT1DM-2-training/540-ws-training.xml\")\n",
    "df544 = parse_XML(\"OHIODATA/OhioT1DM-2-training/544-ws-training.xml\")\n",
    "df552 = parse_XML(\"OHIODATA/OhioT1DM-2-training/552-ws-training.xml\")\n",
    "df567 = parse_XML(\"OHIODATA/OhioT1DM-2-training/567-ws-training.xml\")\n",
    "df584 = parse_XML(\"OHIODATA/OhioT1DM-2-training/584-ws-training.xml\")\n",
    "df596 = parse_XML(\"OHIODATA/OhioT1DM-2-training/596-ws-training.xml\")\n",
    "\n",
    "df540_test = parse_XML(\"OHIODATA/OhioT1DM-2-testing/540-ws-testing.xml\")\n",
    "df544_test = parse_XML(\"OHIODATA/OhioT1DM-2-testing/544-ws-testing.xml\")\n",
    "df552_test = parse_XML(\"OHIODATA/OhioT1DM-2-testing/552-ws-testing.xml\")\n",
    "df567_test = parse_XML(\"OHIODATA/OhioT1DM-2-testing/567-ws-testing.xml\")\n",
    "df584_test = parse_XML(\"OHIODATA/OhioT1DM-2-testing/584-ws-testing.xml\")\n",
    "df596_test = parse_XML(\"OHIODATA/OhioT1DM-2-testing/596-ws-testing.xml\")\n",
    "\n",
    "dfs_train = [df540,df544,df552,df567,df584,df596]\n",
    "dfs_test = [df540_test,df544_test,df552_test,df567_test,df584_test,df596_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HALF HOUR PREDICTIONS\n",
    "for i in range(6):\n",
    "    pred_ARMA = []\n",
    "    data_train = dfs_train[i][\"glucose_level\"].values\n",
    "    data_test = dfs_test[i][\"glucose_level\"].values\n",
    "    data = np.append(data_train,data_test)\n",
    "    for i in range(len(data_test)-20):\n",
    "        model = ARIMA(data[:len(data_train)+i], (2,0,2))        #set the middle value to 0 to get ARMA model\n",
    "        model_fit = model.fit()\n",
    "        pred_ARMA.append(model_fit.predict(len(data_train)+i, len(data_train)+i + 6)[-1])   \n",
    "    mets = get_metrics(data[len(data_train)+6:len(data_train)+6+len(pred_ARMA)], np.array(pred_ARMA)[:])\n",
    "    for i in range(len(mets)):\n",
    "        print(labels[i] + \" : \" + str(mets[i]))\n",
    "        \n",
    "#ONE HOUR PREICTIONS\n",
    "for i in range(len(dfs_test)):\n",
    "    pred_ARMA = []\n",
    "    data_train = dfs_train[i][\"glucose_level\"].values\n",
    "    data_test = dfs_test[i][\"glucose_level\"].values\n",
    "    data = np.append(data_train,data_test)\n",
    "    for i in range(len(data_test)-20):\n",
    "        model = ARIMA(data[:len(data_train)+i], (2,0,2))        #set the middle value to 0 to get ARMA model\n",
    "        model_fit = model.fit()\n",
    "        pred_ARMA.append(model_fit.predict(len(data_train)+i, len(data_train)+i + 12)[-1])   \n",
    "    mets = get_metrics(data[len(data_train)+12:len(data_train)+12+len(pred_ARMA)], np.array(pred_ARMA)[:])\n",
    "    for i in range(len(mets)):\n",
    "        print(labels[i] + \" : \" + str(mets[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is a way to find the p and q values using a grid search method\n",
    "from pmdarima.arima import auto_arima\n",
    "stepwise_model = auto_arima(y, start_p=2, start_q=1,\n",
    "                           max_p=6, max_q=3, m=12,\n",
    "                           start_P=0, seasonal=True,\n",
    "                           d=1, D=1, trace=True,\n",
    "                           error_action='ignore',  \n",
    "                           suppress_warnings=True, \n",
    "                           stepwise=True)\n",
    "print(stepwise_model.aic())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAR (Vectorized Autoregression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_pred2 = []\n",
    "for k in range(len(dfs_train)):                                                                           #runs over each person \n",
    "    data = dfs_train[k].fillna(0)[[\"glucose_level\", \"basal\", \"bolus\",\"exercise\", \"acceleration\"]].values  #using lasso these are some of the significant columns in the predictions\n",
    "    pred = []\n",
    "    for i in range(len(data)-800):                                                                        #Using 800 previous made it stable enough to not throw errors\n",
    "        model = VAR(endog=data[i:800+i])                                                                  #adds new data row each time we get it    \n",
    "        model_fit = model.fit(maxlags=6)\n",
    "        lag_order = model_fit.k_ar\n",
    "        prediction = model_fit.forecast(data[i:800+i][-lag_order:], steps=12)\n",
    "        #The predictions are in the form of an array, where each list represents the predictions of the row.\n",
    "        #so the last row is the step number predictions for each dataframe cloumn value\n",
    "        if prediction[-1][0] < 40 or prediction[-1][0] > 400:                                             #WORKING WITH PREDICTED VALUES OUTSIDE POSSIBLE RANGES\n",
    "            prediction[-1][0] = prediction[-2][0]                                                                      #TODO: try prediction[-2][0]    #used 100 since that is near the average of the glucose values, should use the actual rollign average or a better method like the previous prediction\n",
    "        pred.append(prediction[-1][0]) \n",
    "    saved_pred2.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints the metrics.\n",
    "met_all = np.zeros(13)\n",
    "for o in range(len(dfs_train)):\n",
    "    mets = get_metrics(dfs_train[o][\"glucose_level\"].values[806:], np.array(saved_pred2[o][:-6]))\n",
    "    for i in range(len(mets)):\n",
    "        print(metric_labels[i] + \" : \" + str(mets[i]))\n",
    "    met_all += mets\n",
    "print(met_all/6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the following ANFIS model, an older environment is needed. See link below for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANFIS (Adaptive Neuro Fuzzy Inferance System)\n",
    "\n",
    "https://github.com/tiagoCuervo/TensorANFIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANFIS:\n",
    "    def __init__(self, n_inputs, n_rules, learning_rate=1e-2):\n",
    "        self.n = n_inputs\n",
    "        self.m = n_rules\n",
    "        self.inputs = tf.placeholder(tf.float32, shape=(None, n_inputs))  # Input\n",
    "        self.targets = tf.placeholder(tf.float32, shape=None)  # Desired output\n",
    "        mu = tf.get_variable(\"mu\", [n_rules * n_inputs],\n",
    "                             initializer=tf.random_normal_initializer(0, 1))  # Means of Gaussian MFS\n",
    "        sigma = tf.get_variable(\"sigma\", [n_rules * n_inputs],\n",
    "                                initializer=tf.random_normal_initializer(0, 1))  # Standard deviations of Gaussian MFS\n",
    "        y = tf.get_variable(\"y\", [1, n_rules], initializer=tf.random_normal_initializer(0, 1))  # Sequent centers\n",
    "\n",
    "        self.params = tf.trainable_variables()\n",
    "\n",
    "        self.rul = tf.reduce_prod(\n",
    "            tf.reshape(tf.exp(-0.5 * tf.square(tf.subtract(tf.tile(self.inputs, (1, n_rules)), mu)) / tf.square(sigma)),\n",
    "                       (-1, n_rules, n_inputs)), axis=2)  # Rule activations\n",
    "        # Fuzzy base expansion function:\n",
    "        num = tf.reduce_sum(tf.multiply(self.rul, y), axis=1)\n",
    "        den = tf.clip_by_value(tf.reduce_sum(self.rul, axis=1), 1e-12, 1e12)\n",
    "        self.out = tf.divide(num, den)\n",
    "\n",
    "        #self.loss = tf.losses.huber_loss(self.targets, self.out)  # Loss function computation\n",
    "        # Other loss functions for regression, uncomment to try them:\n",
    "        self.loss = tf.sqrt(tf.losses.mean_squared_error(self.targets, self.out))\n",
    "        # loss = tf.losses.absolute_difference(target, out)\n",
    "        self.optimize = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.loss)  # Optimization step\n",
    "        # Other optimizers, uncomment to try them:\n",
    "        # self.optimize = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(self.loss)\n",
    "        #self.optimize = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(self.loss)\n",
    "        self.init_variables = tf.global_variables_initializer()  # Variable initializer\n",
    "\n",
    "    def infer(self, sess, x, targets=None):\n",
    "        if targets is None:\n",
    "            return sess.run(self.out, feed_dict={self.inputs: x})\n",
    "        else:\n",
    "            return sess.run([self.out, self.loss], feed_dict={self.inputs: x, self.targets: targets})\n",
    "\n",
    "    def train(self, sess, x, targets):\n",
    "        yp, l, _ = sess.run([self.out, self.loss, self.optimize], feed_dict={self.inputs: x, self.targets: targets})\n",
    "        return l, yp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_all = []\n",
    "comb_pred_all = []\n",
    "mets_all = np.zeros(13)\n",
    "for a in range(len(dfs_train)):\n",
    "    combined_all.append(np.append(dfs_train[a][\"glucose_level\"].values,dfs_test[a][\"glucose_level\"].values))\n",
    "\n",
    "\n",
    "#Gets predictions\n",
    "for b in range(len(combined_all)):\n",
    "    #make 12 to 6 for half hour, change 6 to 12 for hour\n",
    "    tf.reset_default_graph()\n",
    "    combined = combined_all[b] - 150\n",
    "    \n",
    "    D = 8  # number of regressors\n",
    "    T = 1  # delay\n",
    "    N = 14500  # Number of points to generate\n",
    "    scale = 150\n",
    "    num_epochs = 300\n",
    "\n",
    "    combined_pred = np.array([])\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    mg_series = combined[-N:]/scale\n",
    "\n",
    "    data = np.zeros((N - T - (D - 1) * T, D))  #creates the split data to train and test\n",
    "    lbls = np.zeros((N - T - (D - 1) * T,))\n",
    "    for t in range((D - 1) * T, N - 12):  \n",
    "        data[t - (D - 1) * T, :] = [mg_series[t - 7 * T],mg_series[t - 6 * T],mg_series[t - 5 * T], mg_series[t - 4 * T],mg_series[t - 3 * T], mg_series[t - 2 * T], mg_series[t -1 * T], mg_series[t]]\n",
    "        lbls[t - (D - 1) * T] = mg_series[t + 12]   \n",
    "    trnData = data[:lbls.size - round(lbls.size * 0.3), :]\n",
    "    trnLbls = lbls[:lbls.size - round(lbls.size * 0.3)]\n",
    "    chkData = data[lbls.size - round(lbls.size * 0.3):, :]\n",
    "    chkLbls = lbls[lbls.size - round(lbls.size * 0.3):]\n",
    "\n",
    "\n",
    "    # ANFIS params and Tensorflow graph initialization\n",
    "    m = 20  # number of rules     usually 16\n",
    "    alpha = 0.01  # learning rate\n",
    "    fis = ANFIS(n_inputs=D, n_rules=m, learning_rate=alpha)\n",
    "\n",
    "    # Initialize session to make computations on the Tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize model parameters\n",
    "        sess.run(fis.init_variables)\n",
    "        trn_costs = []\n",
    "        val_costs = []\n",
    "        for epoch in range(num_epochs):\n",
    "            #  Run an update step\n",
    "            trn_loss, trn_pred = fis.train(sess, trnData, trnLbls)\n",
    "            # Evaluate on validation set\n",
    "            val_pred, val_loss = fis.infer(sess, chkData, chkLbls)\n",
    "            if epoch == num_epochs - 1:\n",
    "                pred = np.vstack((np.expand_dims(trn_pred, 1), np.expand_dims(val_pred, 1)))\n",
    "            trn_costs.append(trn_loss)\n",
    "            val_costs.append(val_loss)\n",
    "        combined_pred = np.append(combined_pred,np.expand_dims(val_pred, 1))   \n",
    "\n",
    "    #In get metrics function comment out the squared argument on RMSE score\n",
    "    mets = get_metrics(combined[-len(combined_pred)+12:][:-200]+150, np.array(combined_pred[:-12]*scale+150)[:-200])\n",
    "    for i in range(len(mets)):\n",
    "        print(metric_labels[i] + \" : \" + str(mets[i]))\n",
    "    comb_pred_all.append(combined_pred)\n",
    "    mets_all +=mets\n",
    "print(mets_all/6)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# While the following models weren't included in the final results, some small work was done for these models. The code is left here as such"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN (Convolutional Neural Net)\n",
    "\n",
    "https://machinelearningmastery.com/how-to-develop-convolutional-neural-network-models-for-time-series-forecasting/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols = [\"date\",\"glucose_level\", \"finger_stick\", \"basal\", \"temp_basal\",\"bolus\",\"meal\",\"sleep\",\n",
    "           \"work\",\"stressors\",\"hypo_event\",\"illness\",\"exercise\",\"basis_heart_rate\",\"basis_gsr\",\n",
    "          \"basis_skin_temperature\",\"basis_air_temperature\",\"basis_steps\",\"basis_sleep\",\"acceleration\"]\n",
    "df_cnn_train = dfs_train[0].fillna(0)\n",
    "df_cnn_test = dfs_test[0].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multivariate multi-step 1d cnn example\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out-1\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    "\n",
    "def create_input(df_vals,start,size):\n",
    "    x_input = []\n",
    "    for i in range(size):\n",
    "        x_input.append(df_vals[start+i])\n",
    "    return np.array(x_input)\n",
    "\n",
    "# define input sequence\n",
    "# convert to [rows, columns] structure\n",
    "N = len(df_cnn_train[\"glucose_level\"].values)\n",
    "\n",
    "in_seq1 = np.array(df_cnn_train[\"glucose_level\"].values[:N-6]).reshape((N-6, 1))\n",
    "in_seq2 = np.array(df_cnn_train[\"finger_stick\"].values[:N-6]).reshape((N-6, 1))\n",
    "in_seq3 = np.array(df_cnn_train[\"basal\"].values[:N-6]).reshape((N-6, 1))\n",
    "in_seq4 = np.array(df_cnn_train[\"temp_basal\"].values[:N-6]).reshape((N-6, 1))\n",
    "in_seq5 = np.array(df_cnn_train[\"bolus\"].values[:N-6]).reshape((N-6, 1))\n",
    "in_seq6 = np.array(df_cnn_train[\"meal\"].values[:N-6]).reshape((N-6, 1))\n",
    "in_seq7 = np.array(df_cnn_train[\"sleep\"].values[:N-6]).reshape((N-6, 1))\n",
    "in_seq8 = np.array(df_cnn_train[\"stressors\"].values[:N-6]).reshape((N-6, 1))\n",
    "in_seq9 = np.array(df_cnn_train[\"hypo_event\"].values[:N-6]).reshape((N-6, 1))\n",
    "in_seq10 = np.array(df_cnn_train[\"illness\"].values[:N-6]).reshape((N-6, 1))\n",
    "in_seq11 = np.array(df_cnn_train[\"exercise\"].values[:N-6]).reshape((N-6, 1))\n",
    "in_seq12 = np.array(df_cnn_train[\"basis_heart_rate\"].values[:N-6]).reshape((N-6, 1))\n",
    "in_seq13 = np.array(df_cnn_train[\"basis_gsr\"].values[:N-6]).reshape((N-6, 1))\n",
    "in_seq14 = np.array(df_cnn_train[\"basis_skin_temperature\"].values[:N-6]).reshape((N-6, 1))\n",
    "in_seq15 = np.array(df_cnn_train[\"basis_air_temperature\"].values[:N-6]).reshape((N-6, 1))\n",
    "in_seq16 = np.array(df_cnn_train[\"basis steps\"].values[:N-6]).reshape((N-6, 1))          \n",
    "in_seq17 = np.array(df_cnn_train[\"basis_sleep\"].values[:N-6]).reshape((N-6, 1))\n",
    "in_seq18 = np.array(df_cnn_train[\"acceleration\"].values[:N-6]).reshape((N-6, 1))\n",
    "in_seq19 = np.array(df_cnn_train[\"work\"].values[:N-6]).reshape((N-6, 1))\n",
    "\n",
    "out_seq = np.array(df_cnn_train[\"glucose_level\"].values[6:N]).reshape((N-6, 1))\n",
    "\n",
    "# horizontally stack columns\n",
    "dataset = np.hstack((in_seq1, in_seq2, in_seq3, in_seq4, in_seq5, in_seq6, in_seq7, in_seq19, in_seq8, in_seq9, in_seq10, in_seq11, in_seq12, in_seq13, in_seq14, in_seq15, in_seq16, in_seq17, in_seq18, out_seq))\n",
    "\n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 30, 6                      #TODO steps in can be hypertuned\n",
    "# convert into input/output\n",
    "X, y = split_sequences(dataset, n_steps_in, n_steps_out)\n",
    "\n",
    "# the dataset knows the number of features, e.g. 2\n",
    "n_features = X.shape[2]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(n_steps_out))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "\n",
    "model.fit(X, y, epochs=500, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets the predictions from the fitted model\n",
    "pred = []\n",
    "for i in range(800):\n",
    "    x_input = create_input(df_cnn_test.values,440+i,30)  #[[df_rnn.values[445+i][1:]]\n",
    "    x_input = x_input.reshape((1, n_steps_in, n_features))\n",
    "    pred.append(model.predict(x_input, verbose=0)[0,-1])\n",
    "    \n",
    "mets = get_metrics(df_cnn_test[\"glucose_level\"].values[482:1282], np.array(pred))\n",
    "for i in range(len(mets)):\n",
    "    print(labels[i] + \" : \" + str(mets[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is a second attempt with more layers\n",
    "n_steps_in, n_steps_out = 30, 6                      #TODO steps in can be hypertuned\n",
    "# convert into input/output\n",
    "X, y = split_sequences(dataset, n_steps_in, n_steps_out)\n",
    "\n",
    "# the dataset knows the number of features, e.g. 2\n",
    "n_features = X.shape[2]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=400, kernel_size=12, activation='relu', input_shape=(n_steps_in, n_features)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(350, activation='relu'))\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(150, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(n_steps_out))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "\n",
    "model.fit(X, y, epochs=2000, verbose=2)\n",
    "\n",
    "pred = []\n",
    "for i in range(800):\n",
    "    x_input = create_input(df_rnn.values,440+i,30)  #[[df_rnn.values[445+i][1:]]\n",
    "    x_input = x_input.reshape((1, n_steps_in, n_features))\n",
    "    pred.append(model.predict(x_input, verbose=0)[0,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "\n",
    "https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate cnn lstm example\n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 24, 6\n",
    "# covert into input/output\n",
    "X, y = split_sequences(dataset, n_steps_in, n_steps_out)\n",
    "# the dataset knows the number of features, e.g. 2\n",
    "n_features = X.shape[2]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
    "model.add(LSTM(100, activation='relu'))\n",
    "model.add(Dense(n_steps_out))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "pred = []\n",
    "for i in range(800):\n",
    "    x_input = create_input(df_rnn.values,440+i,24)  #[[df_rnn.values[445+i][1:]]\n",
    "    x_input = x_input.reshape((1, n_steps_in, n_features))\n",
    "    pred.append(model.predict(x_input, verbose=0)[0,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-LSTM\n",
    "\n",
    "https://machinelearningmastery.com/cnn-long-short-term-memory-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 36, 6\n",
    "# covert into input/output\n",
    "X, y = split_sequences(dataset, n_steps_in, n_steps_out)\n",
    "# the dataset knows the number of features, e.g. 2\n",
    "\n",
    "n_features = X.shape[2]\n",
    "\n",
    "n_features = 19                      #(2**3 , 3 is leftovers)   (2**2, 41, 73 is len X), (19 is features)\n",
    "n_seq = 6\n",
    "n_steps = 6\n",
    "X = X.reshape((X.shape[0], n_seq, n_steps, n_features))\n",
    "\n",
    "# define model\n",
    "model2 = Sequential()\n",
    "model2.add(TimeDistributed(Conv1D(filters=64, kernel_size=6, activation='relu'), input_shape=(6, n_steps, n_features)))\n",
    "model2.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "model2.add(TimeDistributed(Flatten()))\n",
    "model2.add(LSTM(50, activation='relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model2.add(Dense(6))\n",
    "model2.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model2.fit(X, y, epochs=3000, verbose=2)\n",
    "\n",
    "pred = []\n",
    "for i in range(800):\n",
    "    x_input = create_input(df_rnn.values,440+i,36) \n",
    "    x_input = x_input.reshape((1,n_seq, n_steps, n_features))\n",
    "    pred.append(model2.predict(x_input, verbose=0)[0,-1])"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
